{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8698b390",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization & Neural Architecture Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c5e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the correct search path for importing packages\n",
    "import sys\n",
    "sys.path.insert(1, '../PINNLearning')\n",
    "# Surpress unnecessary output from tensorflow package\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Import necessary functions/ packages\n",
    "from training import learning_rate_schedule, oneD_loss\n",
    "from data import gen_data, set_boundaries\n",
    "from models import create_model\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c52ca8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05228c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the boundary conditions\n",
    "x_bc, y_bc = set_boundaries([[0.0], [1.0]], [[1.0], [0.0]])\n",
    "\n",
    "# Create the training data\n",
    "x_train = gen_data(0.0, 1.0, 100)\n",
    "\n",
    "# Pad the bc values and concatenate everthing needed for the loss function into 1 tensor\n",
    "# as the keras tuner expects the input in the form of: (x_train, y_train)\n",
    "x_bc_padded = tf.pad(x_bc, tf.constant([[0, len(x_train) - 2], [0, 0]]))\n",
    "y_bc_padded = tf.pad(y_bc, tf.constant([[0, len(x_train) - 2], [0, 0]]))\n",
    "y_train = tf.concat([x_train, x_bc_padded, y_bc_padded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34114ca9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6125ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of all hyper parameters that need to be searched\n",
    "def search_space(hp):\n",
    "    # defining the learning rate schedule with its searchable parameters\n",
    "    lr_schedule = learning_rate_schedule(\n",
    "        hp.Float('init', min_value=0.5e-3, max_value=5e-3, step=0.1e-3),\n",
    "        hp.Int('steps', min_value=100, max_value=2000),\n",
    "        hp.Float('rate', min_value=0.1, max_value=0.99, step=0.01)\n",
    "    )\n",
    "\n",
    "    # create the model with searchable regularizer, #layers and #weights per layer\n",
    "    l2_penalty = hp.Float('l2_penalty', min_value=0.005, max_value=0.5, step=1e-3)\n",
    "    num_layers = hp.Int('#layers', min_value=2, max_value=10)\n",
    "    weigts = []\n",
    "    for i in range(num_layers):\n",
    "        weigts.append(hp.Int(f'#weights layer {i + 1}', min_value=2, max_value=50))\n",
    "    model = create_model(num_layers, weigts, l2=l2_penalty)\n",
    "\n",
    "    # closure function to convert the expected input format (y_true, y_pred) for \n",
    "    # the loss into a format that can be used in the previously implemented function \n",
    "    def custom_loss(y_true, y_pred):\n",
    "        x_train = y_true[:, 0]\n",
    "        xbc = y_true[:, 1][0:2]\n",
    "        ybc = y_true[:, 2][0:2]\n",
    "        return oneD_loss(model, x_train, xbc, ybc)\n",
    "\n",
    "    # finish the model by assigning the optimizer and the loss\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=lr_schedule),\n",
    "        loss=custom_loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7922b8a1",
   "metadata": {},
   "source": [
    "Kein BOHB vorhanden\n",
    "\n",
    "--> entnehme direkt aus hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea9d57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from ../data/HPO_results/untitled_project/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Defining the tuner as BayesianOptimization with the search space,\n",
    "# the minimizing target, the number of iterations of the\n",
    "# and the folder in which the collected data is saved\n",
    "tuner = kt.BayesianOptimization(\n",
    "    search_space,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,\n",
    "    directory='../data/HPO_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d77096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating callbacks to reduce uneccesarry training time and inculde \n",
    "# output for the tensorboard visuallization below\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(log_dir='../data/HPO_results')\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', baseline=1e-3, patience=0)\n",
    "\n",
    "# Search through the space using the tuner\n",
    "tuner.search(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=60,  # Train every iteration with this number of epochs\n",
    "    validation_split=0.2,  # Use random 20% of the data as validation\n",
    "    callbacks=[tensorboard_cb, stop_early])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7057b10",
   "metadata": {},
   "source": [
    "So the keras tuner, will create automatically the \"maximal\" space and tests points thought it. If eg. the number of layers it returns to me is 2, but I have maximally 5, I can just ignore the values for the other ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56d42a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init: 0.0049\n",
      "steps: 1877\n",
      "rate: 0.58\n",
      "l2_penalty: 0.434\n",
      "#layers: 2\n",
      "#weights layer 1: 32\n",
      "#weights layer 2: 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 24187), started 0:00:04 ago. (Use '!kill 24187' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a880022f0a4e5606\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a880022f0a4e5606\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get and print the best combinations of the HPs\n",
    "best_hps = tuner.get_best_hyperparameters()[0].values\n",
    "for hp in best_hps:\n",
    "    print(f\"{hp}: {best_hps.get(hp)}\")\n",
    "\n",
    "    # Stop printing if the max number of layers is lower than\n",
    "    # the current layer for the number of weights \n",
    "    if hp[-1].isdigit() and int(hp[-1]) >= best_hps.get('#layers'):\n",
    "        break\n",
    "\n",
    "# Load the TensorBoard interactive interface for more information\n",
    "# FYI: Might not show directly, just re-run this cell\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ../data/HPO_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
